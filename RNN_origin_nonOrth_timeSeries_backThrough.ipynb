{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "M2wtTmW-Fyd6",
    "outputId": "2606638b-ac85-45bc-9fd6-a79f42fcca8e"
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import torch\n",
    "import itertools\n",
    "from datetime import datetime\n",
    "import sys\n",
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 224
    },
    "id": "t8KiORdbikgP",
    "outputId": "85a1b842-8d95-4ecb-dc08-5c87feacf0eb"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "DataFrame Shape: 73718 rows, 9 columns\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Unnamed: 0</th>\n",
       "      <th>Year</th>\n",
       "      <th>Month</th>\n",
       "      <th>Day</th>\n",
       "      <th>Date In Fraction Of Year</th>\n",
       "      <th>Number of Sunspots</th>\n",
       "      <th>Standard Deviation</th>\n",
       "      <th>Observations</th>\n",
       "      <th>Indicator</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>1818</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1818.001</td>\n",
       "      <td>-1</td>\n",
       "      <td>-1.0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1</td>\n",
       "      <td>1818</td>\n",
       "      <td>1</td>\n",
       "      <td>2</td>\n",
       "      <td>1818.004</td>\n",
       "      <td>-1</td>\n",
       "      <td>-1.0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2</td>\n",
       "      <td>1818</td>\n",
       "      <td>1</td>\n",
       "      <td>3</td>\n",
       "      <td>1818.007</td>\n",
       "      <td>-1</td>\n",
       "      <td>-1.0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>3</td>\n",
       "      <td>1818</td>\n",
       "      <td>1</td>\n",
       "      <td>4</td>\n",
       "      <td>1818.010</td>\n",
       "      <td>-1</td>\n",
       "      <td>-1.0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>4</td>\n",
       "      <td>1818</td>\n",
       "      <td>1</td>\n",
       "      <td>5</td>\n",
       "      <td>1818.012</td>\n",
       "      <td>-1</td>\n",
       "      <td>-1.0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   Unnamed: 0  Year  Month  Day  Date In Fraction Of Year  Number of Sunspots  \\\n",
       "0           0  1818      1    1                  1818.001                  -1   \n",
       "1           1  1818      1    2                  1818.004                  -1   \n",
       "2           2  1818      1    3                  1818.007                  -1   \n",
       "3           3  1818      1    4                  1818.010                  -1   \n",
       "4           4  1818      1    5                  1818.012                  -1   \n",
       "\n",
       "   Standard Deviation  Observations  Indicator  \n",
       "0                -1.0             0          1  \n",
       "1                -1.0             0          1  \n",
       "2                -1.0             0          1  \n",
       "3                -1.0             0          1  \n",
       "4                -1.0             0          1  "
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "df = pd.read_csv(\"sunspot_data.csv\")\n",
    "print(\"DataFrame Shape: {} rows, {} columns\".format(*df.shape))\n",
    "display(df.head())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "9ph7efbyilFC",
    "outputId": "f4fa5770-e93d-4764-a362-f1588e259135"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Date In Fraction Of Year\n",
       "1818.001   -1\n",
       "1818.004   -1\n",
       "1818.007   -1\n",
       "1818.010   -1\n",
       "1818.012   -1\n",
       "Name: Number of Sunspots, dtype: int64"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "uni_data = df['Number of Sunspots']\n",
    "uni_data.index = df['Date In Fraction Of Year']\n",
    "uni_data.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "id": "GX_gFCQeigm6"
   },
   "outputs": [],
   "source": [
    "TRAIN_SPLIT = 8000\n",
    "uni_data = uni_data.values[:10000]\n",
    "#uni_data = uni_data.astype(np.float64) # convert to float for matrix multiplication later in training\n",
    "'''why need to normalize/standardize -- it only makes for multiple features (we need to standarize them)'''\n",
    "uni_train_mean = uni_data[:TRAIN_SPLIT].mean()\n",
    "uni_train_std = uni_data[:TRAIN_SPLIT].std()\n",
    "uni_data = (uni_data-uni_train_mean)/uni_train_std"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "id": "O-ysxbX3mxaw"
   },
   "outputs": [],
   "source": [
    "# Data loader to create context window\n",
    "def univariate_data(dataset, start_index, end_index, history_size, target_size):\n",
    "    data = []\n",
    "    labels = []\n",
    "\n",
    "    start_index = start_index + history_size\n",
    "    if end_index is None:\n",
    "        end_index = len(dataset) - target_size\n",
    "        \n",
    "    for i in range(start_index, end_index):\n",
    "        indices = range(i-history_size, i)\n",
    "        #Reshape data from (history_size,) to (history_size, 1) to make each input x an array\n",
    "        data.append(np.reshape(dataset[indices], (history_size, 1, 1)))\n",
    "        #data.append(dataset[indices])\n",
    "        targetIndices = range(i, i+target_size)\n",
    "        labels.append(np.reshape(dataset[targetIndices], (target_size, 1, 1)))\n",
    "        #labels.append(dataset[targetIndices])\n",
    "    return np.array(data), np.array(labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "id": "NSMF084TrSTA"
   },
   "outputs": [],
   "source": [
    "univariate_past_history = 100\n",
    "#n-1\n",
    "univariate_future_target = 30\n",
    "\n",
    "x_train_uni, y_train_uni = univariate_data(dataset=uni_data,\n",
    "                                           start_index=0,\n",
    "                                           end_index=TRAIN_SPLIT,\n",
    "                                           history_size=univariate_past_history,\n",
    "                                           target_size=univariate_future_target)\n",
    "x_val_uni, y_val_uni = univariate_data(dataset=uni_data,\n",
    "                                       start_index=TRAIN_SPLIT,\n",
    "                                       end_index=None,\n",
    "                                       history_size=univariate_past_history,\n",
    "                                       target_size=univariate_future_target)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "nYS8zmIKr4et",
    "outputId": "6e9f8b76-c0b5-4ee5-bf0f-d9a164a7ec36"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Single window of past history. Shape: (100, 1, 1)\n"
     ]
    }
   ],
   "source": [
    "print ('Single window of past history. Shape: {}'.format(x_train_uni[0].shape))\n",
    "#print (x_train_uni[0])\n",
    "# print ('\\n Target temperature to predict. Shape: {}'.format(y_train_uni[0].shape))\n",
    "# print (y_train_uni[0])\n",
    "#print(len(x_train_uni[0][0]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "b8dlrTNdrfHn",
    "outputId": "73354bc5-bca3-487f-820a-7db48d91ce51"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(7900, 100, 1, 1)"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x_train_uni.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "P6f6tnMdIVKq",
    "outputId": "e3809905-5d79-414f-9dcd-657188101acb"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(1870, 100, 1, 1)"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x_val_uni.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "id": "Pj0ZiuiX9Ic-"
   },
   "outputs": [],
   "source": [
    "#x_train_uni[:100]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "l8CfE0A_IWcj",
    "outputId": "497ff778-7b27-4168-8bb5-2c886d398b0d"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(7900, 30, 1, 1)"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y_train_uni.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "aSQxYqimrfro",
    "outputId": "980799d4-2e6d-4a5e-c96f-b15b854a21c4"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(1870, 30, 1, 1)"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y_val_uni.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "id": "RCh88g6wGVmA"
   },
   "outputs": [],
   "source": [
    "class Tanh:\n",
    "    def forward(self, x):\n",
    "        return torch.tanh(x)\n",
    "\n",
    "    def backwardWithTanhValue(self, tanh, top_diff):\n",
    "        ## at this activation function layer, we should use * --- which is element-wise multiplication\n",
    "        return (1.0 - torch.square(tanh)) * top_diff"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "id": "aq4ypWRdGE6w"
   },
   "outputs": [],
   "source": [
    "class MultiplyGate:\n",
    "    def forward(self, W, x):\n",
    "        return torch.matmul(W, x)\n",
    "    def backward(self, W, s, dy_pred):\n",
    "    #dy_pred is [f,1]\n",
    "        dW = torch.matmul(dy_pred, torch.transpose(s, 0, 1))\n",
    "        ds = torch.matmul(torch.transpose(W, 0, 1), dy_pred)\n",
    "        return dW, ds\n",
    "\n",
    "class AddGate:\n",
    "    def forward(self, x1, x2):\n",
    "        return x1 + x2\n",
    "    ''' no need of backward func at all since the gradient of addition is 1'''\n",
    "\n",
    "\n",
    "class OutputGate:\n",
    "    def predict(self, mulv, b):\n",
    "        return mulv + b\n",
    "\n",
    "    def lossValue(self, y_pred, y):\n",
    "        #if abs(y - y_pred) > 1000:\n",
    "          # print(\"y\")\n",
    "          # print(y)\n",
    "          # print(\"y_pred\")\n",
    "          # print(y_pred)\n",
    "        return abs(y - y_pred)\n",
    "    \n",
    "    # -sign(y - y_pred) -- note: the order of y and y_pred doesn't matter once you keep in consistent\n",
    "    # this is derative of vector respect with a vector in same shap -- hence, it is element-wise\n",
    "    # ∂(a.T@x)/∂x = a where x is vector\n",
    "    def backward(self, y_pred, y):\n",
    "        return -torch.sign(y - y_pred)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "id": "KttDNhf3KQ97"
   },
   "outputs": [],
   "source": [
    "mulGate = MultiplyGate()\n",
    "addGate = AddGate()\n",
    "activation = Tanh()\n",
    "\n",
    "class RNNLayer:\n",
    "    #x is input -- word vector; s is hidden state vector, U, W, V is matrix\n",
    "    def forward(self, x, prev_s, U, W, V, b_h):\n",
    "        self.input = x\n",
    "        self.mulu = mulGate.forward(U, x)\n",
    "        self.mulw = mulGate.forward(W, prev_s)\n",
    "        self.add = addGate.forward(self.mulw, self.mulu)\n",
    "        self.add = addGate.forward(self.add, b_h)\n",
    "        self.s = activation.forward(self.add)\n",
    "        self.mulv = mulGate.forward(V, self.s)\n",
    "\n",
    "    #all parameters are tensor\n",
    "    def backward(self, prev_s, U, W, V, diff_s, dy_pred):\n",
    "        d_by = dy_pred\n",
    "        dV, dy_predV = mulGate.backward(V, self.s, dy_pred)\n",
    "        # diff_s is not always a vector of 0 --- for back trancate, it is not. value gets acculumated\n",
    "        ds = dy_predV + diff_s\n",
    "        #optimization: replace self.add with self.s directly\n",
    "        dadd = activation.backwardWithTanhValue(self.s, ds)\n",
    "        d_bh = dadd\n",
    "        # the usage of dprev_s???? -- used as ds in the back truncate (its value will be assigned to diff_s and the new dsv will be 0 since dmulv is 0)\n",
    "        dW, dprev_s = mulGate.backward(W, prev_s, dadd)\n",
    "        # calculating dx is a waste  \n",
    "        dU, dx = mulGate.backward(U, self.input, dadd)\n",
    "      \n",
    "        return (dprev_s, dU, dW, dV, d_by, d_bh)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "id": "6WsEuxK-Gd4U"
   },
   "outputs": [],
   "source": [
    "class Model:\n",
    "    def __init__(self, f, feature_dim, hidden_dim=100, bptt_truncate=4):\n",
    "        self.feature_dim = feature_dim\n",
    "        self.hidden_dim = hidden_dim\n",
    "        self.bptt_truncate = bptt_truncate\n",
    "        self.f = f\n",
    "        \n",
    "        self.dtype = torch.float64\n",
    "        self.device = torch.device(\"cuda:0\")\n",
    "        self.tensor = torch.tensor((), dtype=self.dtype, device = self.device)\n",
    "\n",
    "        self.U = torch.from_numpy(np.random.uniform(-np.sqrt(1. / feature_dim), np.sqrt(1. / feature_dim), (hidden_dim, feature_dim))).to(self.device)\n",
    "        self.W = torch.from_numpy(np.random.uniform(-np.sqrt(1. / hidden_dim), np.sqrt(1. / hidden_dim), (hidden_dim, hidden_dim))).to(self.device)\n",
    "        self.V = torch.from_numpy(np.random.uniform(-np.sqrt(1. / hidden_dim), np.sqrt(1. / hidden_dim), (feature_dim, hidden_dim))).to(self.device)\n",
    "        self.b_h = torch.from_numpy(np.random.uniform(-np.sqrt(1. / hidden_dim), np.sqrt(1. / hidden_dim), (hidden_dim, 1))).to(self.device)\n",
    "        self.b_y = torch.from_numpy(np.random.uniform(-np.sqrt(1. / feature_dim), np.sqrt(1. / feature_dim), (feature_dim, 1))).to(self.device)\n",
    "      \n",
    "\n",
    "        #initial orthogonal matrices\n",
    "        u, s, vh = torch.linalg.svd(self.W, full_matrices=False)\n",
    "        self.W = u @ vh\n",
    "        #full_matrices(bool, optional) : If True (default), u and vh have the shapes (…, M, M) and (…, N, N), respectively. \n",
    "        #Otherwise, the shapes are (…, M, K) and (…, K, N), respectively, where K = min(M, N).\n",
    "\n",
    "    # x is (history, features) while y is (targets, features)\n",
    "    def forward_propagation(self, x, y, teacherForcing):\n",
    "        #when there is no prev state, we use tensor of 0\n",
    "        prev_s = self.tensor.new_zeros(self.hidden_dim, 1)\n",
    "        output = OutputGate()\n",
    "        last_predict_y = self.tensor.new_zeros(self.feature_dim, 1)\n",
    "        layers = []\n",
    "        # For each time step in the history window\n",
    "        for t in range(len(x)):\n",
    "            layer = RNNLayer()\n",
    "            #input represents all features in that time stamp -- (features, 1)\n",
    "            input = x[t]\n",
    "            layer.forward(input, prev_s, self.U, self.W, self.V, self.b_h)\n",
    "            layers.append(layer)\n",
    "            prev_s = layer.s\n",
    "            if (t == (len(x) - 1)):\n",
    "              last_predict_y = output.predict(layer.mulv, self.b_y)\n",
    "        \n",
    "        # auto-regressive\n",
    "        # one layer for each time step in target window\n",
    "        yPreds = []\n",
    "        loss = 0.0\n",
    "        # For each time step in the target window\n",
    "        for t in range(len(y)):\n",
    "            yPreds.append(last_predict_y)\n",
    "            loss += output.lossValue(last_predict_y, y[t])\n",
    "\n",
    "            layer = RNNLayer()\n",
    "            if teacherForcing:\n",
    "              '''teacher forcing:'''\n",
    "              input = y[t]\n",
    "            else:\n",
    "              '''original rnn:'''\n",
    "              input = last_predict_y\n",
    "            \n",
    "            layer.forward(input, prev_s, self.U, self.W, self.V, self.b_h)\n",
    "            last_predict_y = output.predict(layer.mulv, self.b_y)\n",
    "\n",
    "            layers.append(layer)\n",
    "            prev_s = layer.s\n",
    "        \n",
    "        lss = loss / float(len(y))\n",
    "        return lss, layers, yPreds, len(x)\n",
    "\n",
    "    def bptt(self, y, layers, yPreds, historyLen):\n",
    "        output = OutputGate()\n",
    "\n",
    "        dU = self.tensor.new_zeros(self.U.shape)\n",
    "        dV = self.tensor.new_zeros(self.V.shape)\n",
    "        dW = self.tensor.new_zeros(self.W.shape)\n",
    "        db_h = self.tensor.new_zeros(self.b_h.shape)\n",
    "        db_y = self.tensor.new_zeros(self.b_y.shape)\n",
    "\n",
    "        diff_s = self.tensor.new_zeros(self.hidden_dim, 1)\n",
    "        #bptt -- timestampes -- future prediction\n",
    "        for t in range(len(y)):\n",
    "            #d|y - y^|/dy^ -- f x 1 shape -- 1/m * -sign(y - y_pred)\n",
    "            dy_pred = output.backward(yPreds[t], y[t])/len(y)\n",
    "            prev_s_t = layers[t+historyLen-1].s\n",
    "            #shape of input is [1,1]\n",
    "            dprev_s, dU_t, dW_t, dV_t, d_by_t, d_bh_t = layers[t].backward(prev_s_t, self.U, self.W, self.V, diff_s, dy_pred)\n",
    "\n",
    "            # the reson of using this inner loop? -- sum up the impacts of s_t, s_t-1, s_t-2, s_t-3, S_t-4 on loss functions\n",
    "            # the reason of initializing dy be 0 vector in shape of f x 1 --- to make ds be 0 and thus make ds == dprev_s (since ds = ds + diff)\n",
    "            dy_hat = self.tensor.new_zeros(self.feature_dim, 1)\n",
    "            # back track last historyLen - 1 layers\n",
    "            for i in range(t + historyLen - 1, 0, -1):\n",
    "                prev_s_i = layers[i-1].s\n",
    "                dprev_s, dU_i, dW_i, dV_i, d_by_i, d_bh_i = layers[i].backward(prev_s_i, self.U, self.W, self.V, dprev_s, dy_hat)\n",
    "                #sum up the impacts of s_t, s_t-1, s_t-2, s_t-3, S_t-4 on loss functions\n",
    "                dU_t += dU_i\n",
    "                dW_t += dW_i\n",
    "                d_bh_t += d_bh_i\n",
    "                \n",
    "            dV += dV_t\n",
    "            db_y += d_by_t\n",
    "            dU += dU_t\n",
    "            dW += dW_t\n",
    "            db_h += d_bh_t\n",
    "            \n",
    "\n",
    "        return (dU, dW, dV, db_h, db_y)\n",
    "\n",
    "    def sgd_step(self, x, y, learning_rate):\n",
    "        #The dtype for an array of arrays will always be object. This is unavoidable because with NumPy only non-jagged n-dimensional arrays can be held in a contiguous memory block.\n",
    "        x = torch.tensor(x, dtype=self.dtype, device=self.device)\n",
    "        y = torch.tensor(y, dtype=self.dtype, device=self.device)\n",
    "        #x is a sentence (aka one example) composed of many words\n",
    "        lss, layers, yPreds, historyLen = self.forward_propagation(x, y, True)\n",
    "        dU, dW, dV, db_h, db_y = self.bptt(y, layers, yPreds, historyLen)\n",
    "        self.U -= learning_rate * dU\n",
    "        self.V -= learning_rate * dV\n",
    "        self.b_h -= learning_rate * db_h\n",
    "        self.b_y -= learning_rate * db_y\n",
    "        self.W -= learning_rate * dW\n",
    "        #don't do orthogonize W at every sgd\n",
    "        #u, s, vh = torch.linalg.svd(self.W, full_matrices=False)\n",
    "        #self.W = u @ vh\n",
    "        \n",
    "        return lss\n",
    "        #This is just for testing in this code\n",
    "        # print(\"W's learning rate is %f\"%(np.linalg.norm(learning_rate * dW)))\n",
    "        # eigvals = np.linalg.eigvals(self.W)\n",
    "        # largestValueIndex = np.argmax(eigvals)\n",
    "        # print(\"W's largest eigenValue is %f\"%(eigvals[largestValueIndex]))\n",
    "\n",
    "    def train(self, X, Y, learning_rate=0.005, nepoch=100, evaluate_loss_after=5):\n",
    "        num_examples_seen = 0\n",
    "        losses = []\n",
    "        for epoch in range(nepoch):\n",
    "            loss = 0\n",
    "\n",
    "            # For each training example...\n",
    "            for i in range(len(Y)):\n",
    "                '''sgd return the loss and accureacy of this sentence/example'''\n",
    "                #print(i)\n",
    "                lss = self.sgd_step(X[i], Y[i], learning_rate)\n",
    "                # print(\"loss of a sample\")\n",
    "                # print(lss)\n",
    "                loss += lss\n",
    "                num_examples_seen += 1\n",
    "\n",
    "            if (epoch % evaluate_loss_after == 0):\n",
    "                #1 place\n",
    "                #loss, accuracy = self.calculate_total_loss_and_predict_accuracy(X, Y)\n",
    "                loss /= len(Y)\n",
    "                losses.append(str(float(loss)))\n",
    "                time = datetime.now().strftime('%Y-%m-%d %H:%M:%S')\n",
    "                print(\"%s: Loss after num_examples_seen=%d epoch=%d: %f\" % (time, num_examples_seen, epoch, loss))\n",
    "                f.write(\"%s: Loss after num_examples_seen=%d epoch=%d: %f\\n\" % (time, num_examples_seen, epoch, loss))\n",
    "                f.flush()\n",
    "                # optional: Adjust the learning rate if loss increases\n",
    "                # if len(losses) > 1 and losses[-1][1] > losses[-2][1]:\n",
    "                #     learning_rate = learning_rate * 0.5\n",
    "                #     print(\"Setting learning rate to %f\" % learning_rate)\n",
    "                sys.stdout.flush()\n",
    "        return losses\n",
    "\n",
    "    def justForwardPropagation(self, x, y):\n",
    "        x = torch.tensor(x, dtype=self.dtype, device=self.device)\n",
    "        y = torch.tensor(y, dtype=self.dtype, device=self.device)\n",
    "        lss, _, _, _ = self.forward_propagation(x, y, False)\n",
    "        return lss\n",
    "\n",
    "    def calculate_testing_loss(self, X, Y):\n",
    "        # For each testing example...\n",
    "        loss = 0\n",
    "        for i in range(len(Y)):\n",
    "            lss = self.justForwardPropagation(X[i], Y[i])\n",
    "            loss += lss\n",
    "        loss /= len(Y)\n",
    "        print(\"Testing loss value=%f\" % (loss))\n",
    "        f.write(\"Testing loss value=%f\\n\" % (loss))\n",
    "        f.flush()\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "id": "BSXtinNBKfQ2"
   },
   "outputs": [],
   "source": [
    "feature_dim = 1\n",
    "hidden_dim = 100"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "id": "aafImhtUAZx_"
   },
   "outputs": [],
   "source": [
    "random_seed = 10 # or any of your favorite number \n",
    "np.random.seed(random_seed)\n",
    "torch.manual_seed(random_seed)\n",
    "torch.cuda.manual_seed(random_seed)\n",
    "torch.backends.cudnn.deterministic = True\n",
    "torch.backends.cudnn.benchmark = False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "z6g-xF1V1Htp",
    "outputId": "cf49d8a3-6165-4693-92f3-4a9db645e71d"
   },
   "outputs": [],
   "source": [
    "f = open(\"RNN_origin_nonOrth_timeSeries_backThrough_loss_log.txt\", \"w\")\n",
    "rnn = Model(f, feature_dim, hidden_dim, univariate_past_history)\n",
    "losses = rnn.train(x_train_uni, y_train_uni, learning_rate=0.0001, nepoch=200, evaluate_loss_after=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "ea9rENQXcURZ"
   },
   "outputs": [],
   "source": [
    "with open(\"RNN_origin_nonOrth_timeSeries_backThrough_loss_array.txt\", \"w\") as txt_file:\n",
    "  txt_file.write(\", \".join(losses))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "oKtj61fHh60n",
    "outputId": "ec85c643-1b5e-4ed8-8a23-206c7248c5d4"
   },
   "outputs": [],
   "source": [
    "rnn.calculate_testing_loss(x_val_uni, y_val_uni)\n",
    "# print(\"Test Data Prediction accuracy=%f\" % (accuracy))\n",
    "# f.write(\"Test Data Prediction accuracy=\" + str(accuracy))\n",
    "f.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Convert the PyTorch tensor to a NumPy array\n",
    "numpy_matrix = rnn.U.cpu().numpy()\n",
    "# Save the NumPy array to a CSV file\n",
    "file_path = 'RNN_origin_nonOrth_timeSeries_backThrough_U.csv'\n",
    "np.savetxt(file_path, numpy_matrix, delimiter=',')\n",
    "\n",
    "numpy_matrix = rnn.W.cpu().numpy()\n",
    "file_path = 'RNN_origin_nonOrth_timeSeries_backThrough_W.csv'\n",
    "np.savetxt(file_path, numpy_matrix, delimiter=',')\n",
    "\n",
    "numpy_matrix = rnn.V.cpu().numpy()\n",
    "file_path = 'RNN_origin_nonOrth_timeSeries_backThrough_V.csv'\n",
    "np.savetxt(file_path, numpy_matrix, delimiter=',')\n",
    "\n",
    "numpy_matrix = rnn.b_y.cpu().numpy()\n",
    "file_path = 'RNN_origin_nonOrth_timeSeries_backThrough_by.csv'\n",
    "np.savetxt(file_path, numpy_matrix, delimiter=',')\n",
    "\n",
    "numpy_matrix = rnn.b_h.cpu().numpy()\n",
    "file_path = 'RNN_origin_nonOrth_timeSeries_backThrough_bh.csv'\n",
    "np.savetxt(file_path, numpy_matrix, delimiter=',')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "6WCGuFh4RCOs"
   },
   "source": [
    "https://songhuiming.github.io/pages/2017/08/20/build-recurrent-neural-network-from-scratch/\n",
    "\n",
    "has the math proof of bptt_truncate"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "8M-4q1dMURjE"
   },
   "source": [
    "https://towardsdatascience.com/implementing-recurrent-neural-network-using-numpy-c359a0a68a67"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "NIifo-AmY328"
   },
   "source": [
    "https://mp.weixin.qq.com/s?__biz=MzIzODExMDE5MA==&mid=2694182661&idx=1&sn=ddfb3f301f5021571992824b21ddcafe&chksm=cc5f0384fb288a92877fbee9c6a1a03ba68e375b1552e762567d71d70105f67aacec7def8c40#rd"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "k6gxSGYSZJmo"
   },
   "source": [
    "https://github.com/nicklashansen/rnn_lstm_from_scratch\n",
    "\n",
    "\n",
    "differen paradigm -- has optimization\n"
   ]
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "provenance": []
  },
  "gpuClass": "standard",
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
