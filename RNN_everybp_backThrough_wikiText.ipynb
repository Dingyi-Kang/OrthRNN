{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: torchtext in ./anaconda3/lib/python3.9/site-packages (0.15.2)\n",
      "Requirement already satisfied: requests in ./anaconda3/lib/python3.9/site-packages (from torchtext) (2.28.1)\n",
      "Requirement already satisfied: tqdm in ./anaconda3/lib/python3.9/site-packages (from torchtext) (4.64.1)\n",
      "Requirement already satisfied: numpy in ./anaconda3/lib/python3.9/site-packages (from torchtext) (1.21.5)\n",
      "Requirement already satisfied: torch==2.0.1 in ./anaconda3/lib/python3.9/site-packages (from torchtext) (2.0.1)\n",
      "Requirement already satisfied: torchdata==0.6.1 in ./anaconda3/lib/python3.9/site-packages (from torchtext) (0.6.1)\n",
      "Requirement already satisfied: jinja2 in ./anaconda3/lib/python3.9/site-packages (from torch==2.0.1->torchtext) (2.11.3)\n",
      "Requirement already satisfied: nvidia-cuda-nvrtc-cu11==11.7.99 in ./anaconda3/lib/python3.9/site-packages (from torch==2.0.1->torchtext) (11.7.99)\n",
      "Requirement already satisfied: nvidia-cufft-cu11==10.9.0.58 in ./anaconda3/lib/python3.9/site-packages (from torch==2.0.1->torchtext) (10.9.0.58)\n",
      "Requirement already satisfied: nvidia-cudnn-cu11==8.5.0.96 in ./anaconda3/lib/python3.9/site-packages (from torch==2.0.1->torchtext) (8.5.0.96)\n",
      "Requirement already satisfied: nvidia-nvtx-cu11==11.7.91 in ./anaconda3/lib/python3.9/site-packages (from torch==2.0.1->torchtext) (11.7.91)\n",
      "Requirement already satisfied: triton==2.0.0 in ./anaconda3/lib/python3.9/site-packages (from torch==2.0.1->torchtext) (2.0.0)\n",
      "Requirement already satisfied: nvidia-cusolver-cu11==11.4.0.1 in ./anaconda3/lib/python3.9/site-packages (from torch==2.0.1->torchtext) (11.4.0.1)\n",
      "Requirement already satisfied: filelock in ./anaconda3/lib/python3.9/site-packages (from torch==2.0.1->torchtext) (3.6.0)\n",
      "Requirement already satisfied: typing-extensions in ./anaconda3/lib/python3.9/site-packages (from torch==2.0.1->torchtext) (4.3.0)\n",
      "Requirement already satisfied: sympy in ./anaconda3/lib/python3.9/site-packages (from torch==2.0.1->torchtext) (1.10.1)\n",
      "Requirement already satisfied: nvidia-curand-cu11==10.2.10.91 in ./anaconda3/lib/python3.9/site-packages (from torch==2.0.1->torchtext) (10.2.10.91)\n",
      "Requirement already satisfied: nvidia-cublas-cu11==11.10.3.66 in ./anaconda3/lib/python3.9/site-packages (from torch==2.0.1->torchtext) (11.10.3.66)\n",
      "Requirement already satisfied: nvidia-cusparse-cu11==11.7.4.91 in ./anaconda3/lib/python3.9/site-packages (from torch==2.0.1->torchtext) (11.7.4.91)\n",
      "Requirement already satisfied: nvidia-cuda-cupti-cu11==11.7.101 in ./anaconda3/lib/python3.9/site-packages (from torch==2.0.1->torchtext) (11.7.101)\n",
      "Requirement already satisfied: nvidia-cuda-runtime-cu11==11.7.99 in ./anaconda3/lib/python3.9/site-packages (from torch==2.0.1->torchtext) (11.7.99)\n",
      "Requirement already satisfied: nvidia-nccl-cu11==2.14.3 in ./anaconda3/lib/python3.9/site-packages (from torch==2.0.1->torchtext) (2.14.3)\n",
      "Requirement already satisfied: networkx in ./anaconda3/lib/python3.9/site-packages (from torch==2.0.1->torchtext) (2.8.4)\n",
      "Requirement already satisfied: urllib3>=1.25 in ./anaconda3/lib/python3.9/site-packages (from torchdata==0.6.1->torchtext) (1.26.11)\n",
      "Requirement already satisfied: wheel in ./anaconda3/lib/python3.9/site-packages (from nvidia-cublas-cu11==11.10.3.66->torch==2.0.1->torchtext) (0.37.1)\n",
      "Requirement already satisfied: setuptools in ./anaconda3/lib/python3.9/site-packages (from nvidia-cublas-cu11==11.10.3.66->torch==2.0.1->torchtext) (63.4.1)\n",
      "Requirement already satisfied: cmake in ./anaconda3/lib/python3.9/site-packages (from triton==2.0.0->torch==2.0.1->torchtext) (3.26.3)\n",
      "Requirement already satisfied: lit in ./anaconda3/lib/python3.9/site-packages (from triton==2.0.0->torch==2.0.1->torchtext) (16.0.3)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in ./anaconda3/lib/python3.9/site-packages (from requests->torchtext) (2022.9.14)\n",
      "Requirement already satisfied: idna<4,>=2.5 in ./anaconda3/lib/python3.9/site-packages (from requests->torchtext) (3.3)\n",
      "Requirement already satisfied: charset-normalizer<3,>=2 in ./anaconda3/lib/python3.9/site-packages (from requests->torchtext) (2.0.4)\n",
      "Requirement already satisfied: MarkupSafe>=0.23 in ./anaconda3/lib/python3.9/site-packages (from jinja2->torch==2.0.1->torchtext) (2.0.1)\n",
      "Requirement already satisfied: mpmath>=0.19 in ./anaconda3/lib/python3.9/site-packages (from sympy->torch==2.0.1->torchtext) (1.2.1)\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "pip install torchtext"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "RYewFLy3epAF",
    "outputId": "8f8fe37e-eff0-4ce4-9684-3fea651e26b7"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: datasets in ./anaconda3/lib/python3.9/site-packages (2.12.0)\n",
      "Requirement already satisfied: requests>=2.19.0 in ./anaconda3/lib/python3.9/site-packages (from datasets) (2.28.1)\n",
      "Requirement already satisfied: multiprocess in ./anaconda3/lib/python3.9/site-packages (from datasets) (0.70.14)\n",
      "Requirement already satisfied: tqdm>=4.62.1 in ./anaconda3/lib/python3.9/site-packages (from datasets) (4.64.1)\n",
      "Requirement already satisfied: responses<0.19 in ./anaconda3/lib/python3.9/site-packages (from datasets) (0.18.0)\n",
      "Requirement already satisfied: fsspec[http]>=2021.11.1 in ./anaconda3/lib/python3.9/site-packages (from datasets) (2022.7.1)\n",
      "Requirement already satisfied: numpy>=1.17 in ./anaconda3/lib/python3.9/site-packages (from datasets) (1.21.5)\n",
      "Requirement already satisfied: aiohttp in ./anaconda3/lib/python3.9/site-packages (from datasets) (3.8.4)\n",
      "Requirement already satisfied: pyyaml>=5.1 in ./anaconda3/lib/python3.9/site-packages (from datasets) (6.0)\n",
      "Requirement already satisfied: pandas in ./anaconda3/lib/python3.9/site-packages (from datasets) (1.4.4)\n",
      "Requirement already satisfied: huggingface-hub<1.0.0,>=0.11.0 in ./anaconda3/lib/python3.9/site-packages (from datasets) (0.11.1)\n",
      "Requirement already satisfied: packaging in ./anaconda3/lib/python3.9/site-packages (from datasets) (21.3)\n",
      "Requirement already satisfied: xxhash in ./anaconda3/lib/python3.9/site-packages (from datasets) (3.2.0)\n",
      "Requirement already satisfied: pyarrow>=8.0.0 in ./anaconda3/lib/python3.9/site-packages (from datasets) (12.0.0)\n",
      "Requirement already satisfied: dill<0.3.7,>=0.3.0 in ./anaconda3/lib/python3.9/site-packages (from datasets) (0.3.6)\n",
      "Requirement already satisfied: typing-extensions>=3.7.4.3 in ./anaconda3/lib/python3.9/site-packages (from huggingface-hub<1.0.0,>=0.11.0->datasets) (4.3.0)\n",
      "Requirement already satisfied: filelock in ./anaconda3/lib/python3.9/site-packages (from huggingface-hub<1.0.0,>=0.11.0->datasets) (3.6.0)\n",
      "Requirement already satisfied: pyparsing!=3.0.5,>=2.0.2 in ./anaconda3/lib/python3.9/site-packages (from packaging->datasets) (3.0.9)\n",
      "Requirement already satisfied: charset-normalizer<3,>=2 in ./anaconda3/lib/python3.9/site-packages (from requests>=2.19.0->datasets) (2.0.4)\n",
      "Requirement already satisfied: idna<4,>=2.5 in ./anaconda3/lib/python3.9/site-packages (from requests>=2.19.0->datasets) (3.3)\n",
      "Requirement already satisfied: urllib3<1.27,>=1.21.1 in ./anaconda3/lib/python3.9/site-packages (from requests>=2.19.0->datasets) (1.26.11)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in ./anaconda3/lib/python3.9/site-packages (from requests>=2.19.0->datasets) (2022.9.14)\n",
      "Requirement already satisfied: aiosignal>=1.1.2 in ./anaconda3/lib/python3.9/site-packages (from aiohttp->datasets) (1.3.1)\n",
      "Requirement already satisfied: attrs>=17.3.0 in ./anaconda3/lib/python3.9/site-packages (from aiohttp->datasets) (21.4.0)\n",
      "Requirement already satisfied: frozenlist>=1.1.1 in ./anaconda3/lib/python3.9/site-packages (from aiohttp->datasets) (1.3.3)\n",
      "Requirement already satisfied: async-timeout<5.0,>=4.0.0a3 in ./anaconda3/lib/python3.9/site-packages (from aiohttp->datasets) (4.0.2)\n",
      "Requirement already satisfied: yarl<2.0,>=1.0 in ./anaconda3/lib/python3.9/site-packages (from aiohttp->datasets) (1.9.2)\n",
      "Requirement already satisfied: multidict<7.0,>=4.5 in ./anaconda3/lib/python3.9/site-packages (from aiohttp->datasets) (6.0.4)\n",
      "Requirement already satisfied: python-dateutil>=2.8.1 in ./anaconda3/lib/python3.9/site-packages (from pandas->datasets) (2.8.2)\n",
      "Requirement already satisfied: pytz>=2020.1 in ./anaconda3/lib/python3.9/site-packages (from pandas->datasets) (2022.1)\n",
      "Requirement already satisfied: six>=1.5 in ./anaconda3/lib/python3.9/site-packages (from python-dateutil>=2.8.1->pandas->datasets) (1.16.0)\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "pip install datasets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "id": "Aiy9_K-Ud-JJ"
   },
   "outputs": [],
   "source": [
    "import torch\n",
    "import math\n",
    "import torchtext\n",
    "import sys\n",
    "import datasets\n",
    "import numpy as np\n",
    "from tqdm import tqdm\n",
    "from datetime import datetime"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 694,
     "referenced_widgets": [
      "0fd4f0e70dc3492f831b78c12bfc7d11",
      "6edad33a30784a14b47085d95632422e",
      "72801c57c09644f9b35e6d1010e0ae16",
      "761b8bdf703c4c03abbbd5cc56f5d0b5",
      "ed21e4234845474ea15c3d411075cf2a",
      "5be1fc46ac3f45398367cb78794e2446",
      "170d5968dc2042268963f93ecce8b911",
      "f1afce97b62a487a9f7b47f49a47a9cc",
      "27689d25627041168afb6dfeb5908b22",
      "27e38b5fb28e417081c0be0f0698910c",
      "59fb6cf5ee994f1ea8aaf5739ee87bb7"
     ]
    },
    "id": "fx3KvgK7efpY",
    "outputId": "0420fada-6b0e-4b4a-c7c1-f1e3a1c66d67"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Found cached dataset wikitext (/home/dkang/.cache/huggingface/datasets/wikitext/wikitext-2-raw-v1/1.0.0/a241db52902eaf2c6aa732210bead40c090019a499ceb13bcbfa3f8ab646a126)\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "1f838f5b3d2c485b9209fe1d4c89b895",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/3 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Loading cached processed dataset at /home/dkang/.cache/huggingface/datasets/wikitext/wikitext-2-raw-v1/1.0.0/a241db52902eaf2c6aa732210bead40c090019a499ceb13bcbfa3f8ab646a126/cache-183251567eb1813b.arrow\n",
      "Loading cached processed dataset at /home/dkang/.cache/huggingface/datasets/wikitext/wikitext-2-raw-v1/1.0.0/a241db52902eaf2c6aa732210bead40c090019a499ceb13bcbfa3f8ab646a126/cache-e110dbe66c3faf3b.arrow\n",
      "Loading cached processed dataset at /home/dkang/.cache/huggingface/datasets/wikitext/wikitext-2-raw-v1/1.0.0/a241db52902eaf2c6aa732210bead40c090019a499ceb13bcbfa3f8ab646a126/cache-abee8172eaf0a427.arrow\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "DatasetDict({\n",
      "    test: Dataset({\n",
      "        features: ['text'],\n",
      "        num_rows: 4358\n",
      "    })\n",
      "    train: Dataset({\n",
      "        features: ['text'],\n",
      "        num_rows: 36718\n",
      "    })\n",
      "    validation: Dataset({\n",
      "        features: ['text'],\n",
      "        num_rows: 3760\n",
      "    })\n",
      "})\n",
      "DatasetDict({\n",
      "    test: Dataset({\n",
      "        features: ['text'],\n",
      "        num_rows: 2080\n",
      "    })\n",
      "    train: Dataset({\n",
      "        features: ['text'],\n",
      "        num_rows: 17034\n",
      "    })\n",
      "    validation: Dataset({\n",
      "        features: ['text'],\n",
      "        num_rows: 1841\n",
      "    })\n",
      "})\n",
      "{'text': \" Partly due to these events , and partly due to the major losses in manpower Gallia suffers towards the end of the war with the Empire , the Nameless are offered a formal position as a squad in the Gallian Army rather than serve as an anonymous shadow force . This is short @-@ lived , however , as following Maximilian 's defeat , Dahau and Calamity Raven move to activate an ancient Valkyrian super weapon within the Empire , kept secret by their benefactor . Without the support of Maximilian or the chance to prove themselves in the war with Gallia , it is Dahau 's last trump card in creating a new Darcsen nation . As an armed Gallian force invading the Empire just following the two nations ' cease @-@ fire would certainly wreck their newfound peace , Kurt decides to once again make his squad the Nameless , asking Crowe to list himself and all under his command as killed @-@ in @-@ action . Now owing allegiance to none other than themselves , the 422nd confronts Dahau and destroys the Valkyrian weapon . Each member then goes their separate ways in order to begin their lives anew . \\n\"}\n",
      " Partly due to these events , and partly due to the major losses in manpower Gallia suffers towards the end of the war with the Empire , the Nameless are offered a formal position as a squad in the Gallian Army rather than serve as an anonymous shadow force . This is short @-@ lived , however , as following Maximilian 's defeat , Dahau and Calamity Raven move to activate an ancient Valkyrian super weapon within the Empire , kept secret by their benefactor . Without the support of Maximilian or the chance to prove themselves in the war with Gallia , it is Dahau 's last trump card in creating a new Darcsen nation . As an armed Gallian force invading the Empire just following the two nations ' cease @-@ fire would certainly wreck their newfound peace , Kurt decides to once again make his squad the Nameless , asking Crowe to list himself and all under his command as killed @-@ in @-@ action . Now owing allegiance to none other than themselves , the 422nd confronts Dahau and destroys the Valkyrian weapon . Each member then goes their separate ways in order to begin their lives anew . \n",
      "\n"
     ]
    }
   ],
   "source": [
    "dataset = datasets.load_dataset('wikitext', 'wikitext-2-raw-v1')\n",
    "print(dataset)\n",
    "\n",
    "def has_at_least_n_words(example):\n",
    "    return len(example['text'].split()) >= 10\n",
    "\n",
    "dataset = dataset.filter(has_at_least_n_words)\n",
    "\n",
    "print(dataset)\n",
    "print(dataset['train'][8])\n",
    "print(dataset['train'][8]['text'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "zrU7shIygDRw",
    "outputId": "da823b12-f086-41b7-8ebd-8428a28de2b9"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Loading cached processed dataset at /home/dkang/.cache/huggingface/datasets/wikitext/wikitext-2-raw-v1/1.0.0/a241db52902eaf2c6aa732210bead40c090019a499ceb13bcbfa3f8ab646a126/cache-b2d3db9cfdb7ca0a.arrow\n",
      "Loading cached processed dataset at /home/dkang/.cache/huggingface/datasets/wikitext/wikitext-2-raw-v1/1.0.0/a241db52902eaf2c6aa732210bead40c090019a499ceb13bcbfa3f8ab646a126/cache-72a4ce18ab49920b.arrow\n",
      "Loading cached processed dataset at /home/dkang/.cache/huggingface/datasets/wikitext/wikitext-2-raw-v1/1.0.0/a241db52902eaf2c6aa732210bead40c090019a499ceb13bcbfa3f8ab646a126/cache-a25052d434f149c5.arrow\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['partly', 'due', 'to', 'these', 'events', ',', 'and', 'partly', 'due', 'to', 'the', 'major', 'losses', 'in', 'manpower', 'gallia', 'suffers', 'towards', 'the', 'end', 'of', 'the', 'war', 'with', 'the', 'empire', ',', 'the', 'nameless', 'are', 'offered', 'a', 'formal', 'position', 'as', 'a', 'squad', 'in', 'the', 'gallian', 'army', 'rather', 'than', 'serve', 'as', 'an', 'anonymous', 'shadow', 'force', '.', 'this', 'is', 'short', '@-@', 'lived', ',', 'however', ',', 'as', 'following', 'maximilian', \"'\", 's', 'defeat', ',', 'dahau', 'and', 'calamity', 'raven', 'move', 'to', 'activate', 'an', 'ancient', 'valkyrian', 'super', 'weapon', 'within', 'the', 'empire', ',', 'kept', 'secret', 'by', 'their', 'benefactor', '.', 'without', 'the', 'support', 'of', 'maximilian', 'or', 'the', 'chance', 'to', 'prove', 'themselves', 'in', 'the', 'war', 'with', 'gallia', ',', 'it', 'is', 'dahau', \"'\", 's', 'last', 'trump', 'card', 'in', 'creating', 'a', 'new', 'darcsen', 'nation', '.', 'as', 'an', 'armed', 'gallian', 'force', 'invading', 'the', 'empire', 'just', 'following', 'the', 'two', 'nations', \"'\", 'cease', '@-@', 'fire', 'would', 'certainly', 'wreck', 'their', 'newfound', 'peace', ',', 'kurt', 'decides', 'to', 'once', 'again', 'make', 'his', 'squad', 'the', 'nameless', ',', 'asking', 'crowe', 'to', 'list', 'himself', 'and', 'all', 'under', 'his', 'command', 'as', 'killed', '@-@', 'in', '@-@', 'action', '.', 'now', 'owing', 'allegiance', 'to', 'none', 'other', 'than', 'themselves', ',', 'the', '422nd', 'confronts', 'dahau', 'and', 'destroys', 'the', 'valkyrian', 'weapon', '.', 'each', 'member', 'then', 'goes', 'their', 'separate', 'ways', 'in', 'order', 'to', 'begin', 'their', 'lives', 'anew', '.']\n"
     ]
    }
   ],
   "source": [
    "'''Tokenizing the Dataset'''\n",
    "#tokenize -- means split the sentence into words\n",
    "tokenizer = torchtext.data.utils.get_tokenizer('basic_english')\n",
    "tokenize_data = lambda example, tokenizer: {'tokens': tokenizer(example['text'])}  \n",
    "#this is the build-in map function of dataset\n",
    "tokenized_dataset = dataset.map(tokenize_data, remove_columns=['text'], fn_kwargs={'tokenizer': tokenizer})\n",
    "print(tokenized_dataset['train'][8]['tokens'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "z4X7j1t3krZo",
    "outputId": "b73c8fc5-6cc0-44d0-b617-76e2a4ab9349"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "8148\n",
      "['<unk>', '<eos>', 'the', ',', '.', 'of', 'and', 'in', 'to', 'a']\n"
     ]
    }
   ],
   "source": [
    "'''Constructing the Vocabulary'''\n",
    "vocab = torchtext.vocab.build_vocab_from_iterator(tokenized_dataset['train']['tokens'], min_freq=20) \n",
    "vocab.insert_token('<unk>', 0)           \n",
    "vocab.insert_token('<eos>', 1)\n",
    "#This means that when a token is not found in the vocabulary, it will be mapped to the <unk> token.            \n",
    "vocab.set_default_index(vocab['<unk>'])   \n",
    "print(len(vocab))                         \n",
    "print(vocab.get_itos()[:10])       "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "id": "HtwZKXy2qZ6L"
   },
   "outputs": [],
   "source": [
    "#Implementing the Dataloaders\n",
    "def get_data(dataset, vocab):\n",
    "    data = [] \n",
    "    sumLength = 0   \n",
    "    count = 0                                               \n",
    "    for example in dataset:\n",
    "        if example['tokens']:                                     \n",
    "            example['tokens'].append('<eos>')             \n",
    "            tokens = [vocab[token] for token in example['tokens']] \n",
    "            data.append(tokens)\n",
    "            count += 1\n",
    "            sumLength += len(tokens)\n",
    "            if count >= 8000:\n",
    "              break \n",
    "\n",
    "    print(sumLength/count)\n",
    "\n",
    "    #for x_training, we include every words except the last one which is <eos>\n",
    "    X_train = np.asarray([[token_index for token_index in sent[:-1]] for sent in data])\n",
    "    #for y_training, we include every words except the first one since we don't predict the first one\n",
    "    Y_train = np.asarray([[token_index for token_index in sent[1:]] for sent in data])\n",
    "    \n",
    "    print(\"X_train shape: \" + str(X_train.shape))\n",
    "    print(\"y_train shape: \" + str(Y_train.shape))\n",
    "\n",
    "    # Print an training data example\n",
    "    x_example, y_example = X_train[17], Y_train[17]\n",
    "    print(\"x:\\n%s\\n%s\" % (\" \".join([vocab.get_itos()[x] for x in x_example]), x_example))\n",
    "    print(\"\\ny:\\n%s\\n%s\" % (\" \".join([vocab.get_itos()[x] for x in y_example]), y_example))\n",
    "\n",
    "    return X_train, Y_train                                       "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "dpIonSckDb4w",
    "outputId": "edf059a7-9daf-4f59-aac0-8fd18bef4bcd"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "120.776625\n",
      "X_train shape: (8000,)\n",
      "y_train shape: (8000,)\n",
      "x:\n",
      "in a <unk> of the <unk> demo , ryan <unk> of ign was left excited as to where the game would go after completing the demo , along with <unk> the improved <unk> over valkyria chronicles ii . <unk> ' s richard <unk> was highly positive about the game , citing is story as a return to form after valkyria chronicles ii and its gameplay being the best in the series . his main <unk> were its length and gameplay <unk> , along with expressing <unk> that it would not be <unk> .\n",
      "[7, 9, 0, 5, 2, 0, 7417, 3, 2945, 0, 5, 2512, 10, 250, 7240, 14, 8, 92, 2, 68, 63, 637, 45, 4150, 2, 7417, 3, 162, 18, 0, 2, 2060, 0, 66, 3877, 3894, 311, 4, 0, 11, 15, 1117, 0, 10, 1279, 923, 73, 2, 68, 3, 3217, 23, 329, 14, 9, 546, 8, 282, 45, 3877, 3894, 311, 6, 43, 2536, 96, 2, 175, 7, 2, 94, 4, 27, 255, 0, 29, 43, 850, 6, 2536, 0, 3, 162, 18, 7947, 0, 16, 24, 63, 40, 34, 0, 4]\n",
      "\n",
      "y:\n",
      "a <unk> of the <unk> demo , ryan <unk> of ign was left excited as to where the game would go after completing the demo , along with <unk> the improved <unk> over valkyria chronicles ii . <unk> ' s richard <unk> was highly positive about the game , citing is story as a return to form after valkyria chronicles ii and its gameplay being the best in the series . his main <unk> were its length and gameplay <unk> , along with expressing <unk> that it would not be <unk> . <eos>\n",
      "[9, 0, 5, 2, 0, 7417, 3, 2945, 0, 5, 2512, 10, 250, 7240, 14, 8, 92, 2, 68, 63, 637, 45, 4150, 2, 7417, 3, 162, 18, 0, 2, 2060, 0, 66, 3877, 3894, 311, 4, 0, 11, 15, 1117, 0, 10, 1279, 923, 73, 2, 68, 3, 3217, 23, 329, 14, 9, 546, 8, 282, 45, 3877, 3894, 311, 6, 43, 2536, 96, 2, 175, 7, 2, 94, 4, 27, 255, 0, 29, 43, 850, 6, 2536, 0, 3, 162, 18, 7947, 0, 16, 24, 63, 40, 34, 0, 4, 1]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_127934/279285675.py:19: VisibleDeprecationWarning: Creating an ndarray from ragged nested sequences (which is a list-or-tuple of lists-or-tuples-or ndarrays with different lengths or shapes) is deprecated. If you meant to do this, you must specify 'dtype=object' when creating the ndarray.\n",
      "  X_train = np.asarray([[token_index for token_index in sent[:-1]] for sent in data])\n",
      "/tmp/ipykernel_127934/279285675.py:21: VisibleDeprecationWarning: Creating an ndarray from ragged nested sequences (which is a list-or-tuple of lists-or-tuples-or ndarrays with different lengths or shapes) is deprecated. If you meant to do this, you must specify 'dtype=object' when creating the ndarray.\n",
      "  Y_train = np.asarray([[token_index for token_index in sent[1:]] for sent in data])\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "115.16153846153846\n",
      "X_train shape: (2080,)\n",
      "y_train shape: (2080,)\n",
      "x:\n",
      "the an <unk> rebellion began in december <unk> , and was not completely <unk> for almost eight years . it caused enormous <unk> to chinese society the census of <unk> recorded 52 @ . @ 9 million people , but ten years later , the census <unk> just 16 @ . @ 9 million , the remainder having been displaced or killed . during this time , du <unk> led a largely <unk> life <unk> by wars , associated <unk> and imperial <unk> . this period of <unk> was the making of du <unk> as a poet even <unk> <unk> has written that , what he saw around him — the lives of his family , <unk> , and <unk> – what he heard , and what he hoped for or feared from the progress of various campaigns — these became the enduring themes of his poetry . even when he learned of the death of his youngest child , he turned to the suffering of others in his poetry instead of <unk> upon his own <unk> . du <unk> wrote\n",
      "[2, 30, 0, 5056, 135, 7, 276, 0, 3, 6, 10, 40, 1382, 0, 17, 552, 452, 102, 4, 24, 506, 5443, 0, 8, 861, 722, 2, 4534, 5, 0, 308, 2813, 20, 4, 20, 201, 165, 150, 3, 38, 565, 102, 75, 3, 2, 4534, 0, 241, 338, 20, 4, 20, 201, 165, 3, 2, 2545, 345, 52, 4821, 48, 567, 4, 57, 36, 59, 3, 4021, 0, 298, 9, 989, 0, 189, 0, 19, 2809, 3, 1128, 0, 6, 2296, 0, 4, 36, 317, 5, 0, 10, 2, 404, 5, 4021, 0, 14, 9, 2186, 248, 0, 0, 51, 326, 16, 3, 185, 28, 680, 158, 78, 127, 2, 1571, 5, 27, 222, 3, 0, 3, 6, 0, 46, 185, 28, 2295, 3, 6, 185, 28, 2899, 17, 48, 5991, 25, 2, 2543, 5, 472, 5553, 127, 101, 121, 2, 7233, 1624, 5, 27, 1901, 4, 248, 58, 28, 2202, 5, 2, 278, 5, 27, 4199, 864, 3, 28, 836, 8, 2, 2906, 5, 484, 7, 27, 1901, 427, 5, 0, 494, 27, 274, 0, 4, 4021, 0, 220]\n",
      "\n",
      "y:\n",
      "an <unk> rebellion began in december <unk> , and was not completely <unk> for almost eight years . it caused enormous <unk> to chinese society the census of <unk> recorded 52 @ . @ 9 million people , but ten years later , the census <unk> just 16 @ . @ 9 million , the remainder having been displaced or killed . during this time , du <unk> led a largely <unk> life <unk> by wars , associated <unk> and imperial <unk> . this period of <unk> was the making of du <unk> as a poet even <unk> <unk> has written that , what he saw around him — the lives of his family , <unk> , and <unk> – what he heard , and what he hoped for or feared from the progress of various campaigns — these became the enduring themes of his poetry . even when he learned of the death of his youngest child , he turned to the suffering of others in his poetry instead of <unk> upon his own <unk> . du <unk> wrote <eos>\n",
      "[30, 0, 5056, 135, 7, 276, 0, 3, 6, 10, 40, 1382, 0, 17, 552, 452, 102, 4, 24, 506, 5443, 0, 8, 861, 722, 2, 4534, 5, 0, 308, 2813, 20, 4, 20, 201, 165, 150, 3, 38, 565, 102, 75, 3, 2, 4534, 0, 241, 338, 20, 4, 20, 201, 165, 3, 2, 2545, 345, 52, 4821, 48, 567, 4, 57, 36, 59, 3, 4021, 0, 298, 9, 989, 0, 189, 0, 19, 2809, 3, 1128, 0, 6, 2296, 0, 4, 36, 317, 5, 0, 10, 2, 404, 5, 4021, 0, 14, 9, 2186, 248, 0, 0, 51, 326, 16, 3, 185, 28, 680, 158, 78, 127, 2, 1571, 5, 27, 222, 3, 0, 3, 6, 0, 46, 185, 28, 2295, 3, 6, 185, 28, 2899, 17, 48, 5991, 25, 2, 2543, 5, 472, 5553, 127, 101, 121, 2, 7233, 1624, 5, 27, 1901, 4, 248, 58, 28, 2202, 5, 2, 278, 5, 27, 4199, 864, 3, 28, 836, 8, 2, 2906, 5, 484, 7, 27, 1901, 427, 5, 0, 494, 27, 274, 0, 4, 4021, 0, 220, 1]\n",
      "(8000,)\n",
      "(8000,)\n",
      "(2080,)\n",
      "(2080,)\n"
     ]
    }
   ],
   "source": [
    "'''mean of length of sentence is 120'''\n",
    "X_train, Y_train = get_data(tokenized_dataset['train'], vocab)\n",
    "# X_validation, Y_validation = get_data(tokenized_dataset['validation'], vocab)\n",
    "'''mean of length of sentence is 115'''\n",
    "X_test, Y_test = get_data(tokenized_dataset['test'], vocab)\n",
    "\n",
    "# X_test = np.concatenate((X_validation, X_test_pre), axis=0)\n",
    "# Y_test = np.concatenate((Y_validation, Y_test_pre), axis=0)\n",
    "\n",
    "print(X_train.shape)\n",
    "print(Y_train.shape)\n",
    "# print(X_validation.shape)\n",
    "# print(Y_validation.shape)\n",
    "# print(X_test_pre.shape)\n",
    "# print(Y_test_pre.shape)\n",
    "print(X_test.shape)\n",
    "print(Y_test.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "Q0nRB11IM-F3",
    "outputId": "feddacd2-ec1b-434c-f7c0-094211f6617b"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "x:\n",
      "after defeating <unk> , donald receives a more complete map . in india , donald enters the palace of the <unk> , where she challenges him to defeat the tiger in her garden in exchange for a <unk> <unk> . donald <unk> and receives the <unk> <unk> , which is the key to open a temple in egypt . donald is able to solve the <unk> of the <unk> using the note <unk> had given him , and <unk> the <unk> of ra before <unk> in a mine <unk> . from there , he <unk> to the south pole , where he finds a key <unk> in ice , and uses the <unk> of ra to <unk> the ice and <unk> the key . the key <unk> the hold of a <unk> ship , which contains an ancient diary with the secret to <unk> the treasure . the ship is <unk> by <unk> , and the <unk> captain sends donald below <unk> to get <unk> of them . after defeating a <unk> <unk> warrior , donald returns to the deck , where the captain <unk> him that the diary is hidden in ice near the south pole , and gives him an ancient <unk> <unk> that <unk> to flying <unk> . donald then returns to the south pole , <unk> a ride on one of pete ' s bird <unk> to reach the diary .\n",
      "[45, 3182, 0, 3, 5209, 4046, 9, 62, 796, 3569, 4, 7, 822, 3, 5209, 3905, 2, 2460, 5, 2, 0, 3, 92, 54, 5554, 78, 8, 1092, 2, 7839, 7, 47, 3560, 7, 2691, 17, 9, 0, 0, 4, 5209, 0, 6, 4046, 2, 0, 0, 3, 32, 23, 2, 1021, 8, 414, 9, 957, 7, 2289, 4, 5209, 23, 522, 8, 7119, 2, 0, 5, 2, 0, 434, 2, 1795, 0, 31, 387, 78, 3, 6, 0, 2, 0, 5, 4954, 91, 0, 7, 9, 2766, 0, 4, 25, 80, 3, 28, 0, 8, 2, 108, 3577, 3, 92, 28, 2058, 9, 1021, 0, 7, 2183, 3, 6, 1274, 2, 0, 5, 4954, 8, 0, 2, 2183, 6, 0, 2, 1021, 4, 2, 1021, 0, 2, 1311, 5, 9, 0, 365, 3, 32, 1014, 30, 1160, 7928, 18, 2, 1643, 8, 0, 2, 6719, 4, 2, 365, 23, 0, 19, 0, 3, 6, 2, 0, 863, 8081, 5209, 1253, 0, 8, 642, 0, 5, 95, 4, 45, 3182, 9, 0, 0, 3998, 3, 5209, 3155, 8, 2, 3460, 3, 92, 2, 863, 0, 78, 16, 2, 7928, 23, 4233, 7, 2183, 261, 2, 108, 3577, 3, 6, 1932, 78, 30, 1160, 0, 0, 16, 0, 8, 1567, 0, 4, 5209, 112, 3155, 8, 2, 108, 3577, 3, 0, 9, 3757, 13, 41, 5, 5613, 11, 15, 1178, 0, 8, 1245, 2, 7928, 4]\n",
      "\n",
      "y:\n",
      "defeating <unk> , donald receives a more complete map . in india , donald enters the palace of the <unk> , where she challenges him to defeat the tiger in her garden in exchange for a <unk> <unk> . donald <unk> and receives the <unk> <unk> , which is the key to open a temple in egypt . donald is able to solve the <unk> of the <unk> using the note <unk> had given him , and <unk> the <unk> of ra before <unk> in a mine <unk> . from there , he <unk> to the south pole , where he finds a key <unk> in ice , and uses the <unk> of ra to <unk> the ice and <unk> the key . the key <unk> the hold of a <unk> ship , which contains an ancient diary with the secret to <unk> the treasure . the ship is <unk> by <unk> , and the <unk> captain sends donald below <unk> to get <unk> of them . after defeating a <unk> <unk> warrior , donald returns to the deck , where the captain <unk> him that the diary is hidden in ice near the south pole , and gives him an ancient <unk> <unk> that <unk> to flying <unk> . donald then returns to the south pole , <unk> a ride on one of pete ' s bird <unk> to reach the diary . <eos>\n",
      "[3182, 0, 3, 5209, 4046, 9, 62, 796, 3569, 4, 7, 822, 3, 5209, 3905, 2, 2460, 5, 2, 0, 3, 92, 54, 5554, 78, 8, 1092, 2, 7839, 7, 47, 3560, 7, 2691, 17, 9, 0, 0, 4, 5209, 0, 6, 4046, 2, 0, 0, 3, 32, 23, 2, 1021, 8, 414, 9, 957, 7, 2289, 4, 5209, 23, 522, 8, 7119, 2, 0, 5, 2, 0, 434, 2, 1795, 0, 31, 387, 78, 3, 6, 0, 2, 0, 5, 4954, 91, 0, 7, 9, 2766, 0, 4, 25, 80, 3, 28, 0, 8, 2, 108, 3577, 3, 92, 28, 2058, 9, 1021, 0, 7, 2183, 3, 6, 1274, 2, 0, 5, 4954, 8, 0, 2, 2183, 6, 0, 2, 1021, 4, 2, 1021, 0, 2, 1311, 5, 9, 0, 365, 3, 32, 1014, 30, 1160, 7928, 18, 2, 1643, 8, 0, 2, 6719, 4, 2, 365, 23, 0, 19, 0, 3, 6, 2, 0, 863, 8081, 5209, 1253, 0, 8, 642, 0, 5, 95, 4, 45, 3182, 9, 0, 0, 3998, 3, 5209, 3155, 8, 2, 3460, 3, 92, 2, 863, 0, 78, 16, 2, 7928, 23, 4233, 7, 2183, 261, 2, 108, 3577, 3, 6, 1932, 78, 30, 1160, 0, 0, 16, 0, 8, 1567, 0, 4, 5209, 112, 3155, 8, 2, 108, 3577, 3, 0, 9, 3757, 13, 41, 5, 5613, 11, 15, 1178, 0, 8, 1245, 2, 7928, 4, 1]\n"
     ]
    }
   ],
   "source": [
    "x_example, y_example = X_test[1086], Y_test[1086]\n",
    "print(\"x:\\n%s\\n%s\" % (\" \".join([vocab.get_itos()[x] for x in x_example]), x_example))\n",
    "print(\"\\ny:\\n%s\\n%s\" % (\" \".join([vocab.get_itos()[x] for x in y_example]), y_example))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "id": "cuIu3WNoddFk"
   },
   "outputs": [],
   "source": [
    "class Tanh:\n",
    "    def forward(self, x):\n",
    "        return torch.tanh(x)\n",
    "\n",
    "    def backwardWithTanhValue(self, tanh, top_diff):\n",
    "        ## at this activation function layer, we should use * --- which is element-wise multiplication\n",
    "        return (1.0 - torch.square(tanh)) * top_diff\n",
    "\n",
    "class Softmax:\n",
    "    def predict(self, mulv, b_y):\n",
    "        x = mulv + b_y\n",
    "        #softmax(x, dim=-1) The dim argument is required unless your input tensor is a vector\n",
    "        return torch.softmax(x, dim = 0)\n",
    "\n",
    "    def lossWithSoftmaxProb(self, probs, y):\n",
    "        return -torch.log(probs[y])\n",
    "\n",
    "    def diffWithSoftmaxProb(self, probs, y):\n",
    "        probs[y] -= 1.0\n",
    "        #return y^ - y\n",
    "        return probs\n",
    "\n",
    "class MultiplyGate:\n",
    "    def forward(self, W, x):\n",
    "        return torch.matmul(W, x)\n",
    "    def backward(self, W, x, dz):\n",
    "        #x is state\n",
    "        #so we don't need to transpose it anymore, which is equavilent to dz * transpose(x)\n",
    "        dW = torch.matmul(torch.transpose(dz.unsqueeze(0), 0, 1), x.unsqueeze(0))\n",
    "        dx = torch.matmul(torch.transpose(W, 0, 1), dz)        \n",
    "        return dW, dx\n",
    "\n",
    "class AddGate:\n",
    "    def forward(self, x1, x2):\n",
    "        return x1 + x2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "id": "_CJFhYC8aCLb"
   },
   "outputs": [],
   "source": [
    "mulGate = MultiplyGate()\n",
    "addGate = AddGate()\n",
    "activation = Tanh()\n",
    "\n",
    "class RNNLayer:\n",
    "    #x is input -- word vector; s is hidden state vector, U, W, V is matrix\n",
    "    def forward(self, x, prev_s, U, W, V, b_h):\n",
    "        self.input = x\n",
    "        self.mulu = mulGate.forward(U, x)\n",
    "        self.mulw = mulGate.forward(W, prev_s)\n",
    "        self.add = addGate.forward(self.mulw, self.mulu)\n",
    "        self.add = addGate.forward(self.add, b_h)\n",
    "        self.s = activation.forward(self.add)\n",
    "        self.mulv = mulGate.forward(V, self.s)\n",
    "    \n",
    "    #all parameters are tensor\n",
    "    def backward(self, prev_s, U, W, V, diff_s, dy_pred):\n",
    "        d_by = dy_pred\n",
    "        dV, dy_predV = mulGate.backward(V, self.s, dy_pred)\n",
    "        # diff_s is not always a vector of 0 --- for back trancate, it is not. value gets acculumated\n",
    "        ds = dy_predV + diff_s\n",
    "        #optimization: replace self.add with self.s directly\n",
    "        dadd = activation.backwardWithTanhValue(self.s, ds)\n",
    "        d_bh = dadd\n",
    "        '''no need of this add.backward step since the gradient of addition is 1'''\n",
    "        # the usage of dprev_s???? -- used as ds in the back truncate (its value will be assigned to diff_s and the new dsv will be 0 since dmulv is 0)\n",
    "        dW, dprev_s = mulGate.backward(W, prev_s, dadd)\n",
    "        dU, dx = mulGate.backward(U, self.input, dadd)\n",
    "        return (dprev_s, dU, dW, dV, d_by, d_bh)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "id": "z4Y4bAMTaOny"
   },
   "outputs": [],
   "source": [
    "class Model:\n",
    "    def __init__(self, f, word_dim, hidden_dim=100, bptt_truncate=4):\n",
    "        self.word_dim = word_dim\n",
    "        self.hidden_dim = hidden_dim\n",
    "        self.bptt_truncate = bptt_truncate\n",
    "        self.f = f\n",
    "        \n",
    "        self.dtype = torch.float64\n",
    "        self.device = torch.device(\"cuda:0\")\n",
    "        self.tensor = torch.tensor((), dtype=self.dtype, device = self.device)\n",
    "\n",
    "        self.U = torch.from_numpy(np.random.uniform(-np.sqrt(1. / word_dim), np.sqrt(1. / word_dim), (hidden_dim, word_dim))).to(self.device)\n",
    "        self.W = torch.from_numpy(np.random.uniform(-np.sqrt(1. / hidden_dim), np.sqrt(1. / hidden_dim), (hidden_dim, hidden_dim))).to(self.device)\n",
    "        self.V = torch.from_numpy(np.random.uniform(-np.sqrt(1. / hidden_dim), np.sqrt(1. / hidden_dim), (word_dim, hidden_dim))).to(self.device)\n",
    "        self.b_h = self.tensor.new_zeros(hidden_dim)\n",
    "        #this b_y in fact represents the frequency of words in training dataset\n",
    "        self.b_y = self.tensor.new_zeros(word_dim)\n",
    "      \n",
    "        #initial orthogonal matrices\n",
    "        u, s, vh = torch.linalg.svd(self.W, full_matrices=False)\n",
    "        self.W = u @ vh\n",
    "        #full_matrices(bool, optional) : If True (default), u and vh have the shapes (…, M, M) and (…, N, N), respectively. \n",
    "        #Otherwise, the shapes are (…, M, K) and (…, K, N), respectively, where K = min(M, N).\n",
    "    '''\n",
    "    forward propagation (predicting word probabilities and calculate the loss/accuracy)\n",
    "    for example x = [0, 179, 341, 416], then its y = [179, 341, 416, 1]\n",
    "    x is a single sentence which is an array of indexes of words\n",
    "    x[i] is the index of the word in the words vocabulary\n",
    "    '''\n",
    "    def forward_propagation(self, x, y):\n",
    "        assert len(x) == len(y)\n",
    "        # The total number of time steps\n",
    "        T = len(x)\n",
    "        layers = []\n",
    "        prev_s = self.tensor.new_zeros(self.hidden_dim)\n",
    "        output = Softmax()\n",
    "        hit = 0\n",
    "        loss = 0.0\n",
    "        ys_pred_probs = []\n",
    "        # For each time step in the sentence\n",
    "        for t in range(T):\n",
    "            layer = RNNLayer()\n",
    "            #input still represents the input word as a probality vector in the vocabulary\n",
    "            # change this input vector from np to tensor\n",
    "            input = self.tensor.new_zeros(self.word_dim)\n",
    "            '''no need of not using teacher forceing - or it will be tough for begining words'''\n",
    "            input[x[t]] = 1\n",
    "            layer.forward(input, prev_s, self.U, self.W, self.V, self.b_h)\n",
    "            y_pred_prob = output.predict(layer.mulv, self.b_y)\n",
    "            loss += output.lossWithSoftmaxProb(y_pred_prob, y[t])\n",
    "            #output.predict -- softmax(mulv)\n",
    "            if torch.argmax(y_pred_prob) == y[t]:\n",
    "              hit += 1\n",
    "            prev_s = layer.s\n",
    "\n",
    "            ys_pred_probs.append(y_pred_prob)\n",
    "            layers.append(layer)\n",
    "        lss = loss / float(len(y))\n",
    "        acc = hit / float(len(y))\n",
    "        return lss, acc, layers, ys_pred_probs\n",
    "\n",
    "    def bptt(self, y, layers, ys_pred_probs):\n",
    "        output = Softmax()\n",
    "\n",
    "        dU = self.tensor.new_zeros(self.U.shape)\n",
    "        dV = self.tensor.new_zeros(self.V.shape)\n",
    "        dW = self.tensor.new_zeros(self.W.shape)\n",
    "        db_h = self.tensor.new_zeros(self.b_h.shape)\n",
    "        db_y = self.tensor.new_zeros(self.b_y.shape)\n",
    "\n",
    "        prev_s_t = self.tensor.new_zeros(self.hidden_dim)\n",
    "        diff_s = self.tensor.new_zeros(self.hidden_dim)\n",
    "\n",
    "        for t in range(len(layers)):\n",
    "            #y^ - y -- dy_pred\n",
    "            dy_pred = output.diffWithSoftmaxProb(ys_pred_probs[t], y[t])\n",
    "\n",
    "            dprev_s, dU_t, dW_t, dV_t, d_by_t, d_bh_t = layers[t].backward(prev_s_t, self.U, self.W, self.V, diff_s, dy_pred)\n",
    "            prev_s_t = layers[t].s\n",
    "\n",
    "            # the reson of using this inner loop? -- sum up the impacts of s_t, s_t-1, s_t-2, s_t-3, S_t-4 on loss functions\n",
    "            # the reason of initializing dmulv be 0 vector --- to make ds == dprev_s; since ds = ds + diff\n",
    "            dy_hat = self.tensor.new_zeros(self.word_dim)\n",
    "            #for i in range(t-1, max(-1, t-self.bptt_truncate-1), -1):\n",
    "            for i in range(t-1, -1, -1):\n",
    "                prev_s_i = self.tensor.new_zeros(self.hidden_dim) if i == 0 else layers[i-1].s\n",
    "                dprev_s, dU_i, dW_i, dV_i, d_by_i, d_bh_i = layers[i].backward(prev_s_i, self.U, self.W, self.V, dprev_s, dy_hat)\n",
    "                #sum up the impacts of s_t, s_t-1, s_t-2, s_t-3, S_t-4 on loss functions\n",
    "                dU_t += dU_i\n",
    "                dW_t += dW_i\n",
    "                d_bh_t += d_bh_i\n",
    "\n",
    "            dV += dV_t\n",
    "            db_y += d_by_t\n",
    "            dU += dU_t\n",
    "            dW += dW_t\n",
    "            db_h += d_bh_t\n",
    "\n",
    "        return (dU, dW, dV, db_h, db_y)\n",
    "\n",
    "    def sgd_step(self, x, y, learning_rate):\n",
    "        #The dtype for an array of arrays will always be object. This is unavoidable because with NumPy only non-jagged n-dimensional arrays can be held in a contiguous memory block.\n",
    "        #Notice your constituent arrays are already of int dtype:\n",
    "        x = torch.tensor(x, dtype=torch.int32, device=self.device)\n",
    "        y = torch.tensor(y, dtype=torch.int32, device=self.device)\n",
    "        #x is a sentence (aka one example) composed of many words\n",
    "        lss, acc, layers, ys_pred_probs = self.forward_propagation(x, y)\n",
    "        dU, dW, dV, db_h, db_y = self.bptt(y, layers, ys_pred_probs)\n",
    "        self.U -= learning_rate * dU\n",
    "        self.V -= learning_rate * dV\n",
    "        self.b_h -= learning_rate * db_h\n",
    "        self.b_y -= learning_rate * db_y\n",
    "        self.W -= learning_rate * dW\n",
    "        #orthogonize W at every sgd\n",
    "        u, s, vh = torch.linalg.svd(self.W, full_matrices=False)\n",
    "        self.W = u @ vh\n",
    "        \n",
    "        return lss, acc\n",
    "        #This is just for testing in this code\n",
    "        # print(\"W's learning rate is %f\"%(np.linalg.norm(learning_rate * dW)))\n",
    "        # eigvals = np.linalg.eigvals(self.W)\n",
    "        # largestValueIndex = np.argmax(eigvals)\n",
    "        # print(\"W's largest eigenValue is %f\"%(eigvals[largestValueIndex]))\n",
    "        \n",
    "    def train(self, X, Y, learning_rate=0.005, nepoch=100, evaluate_loss_after=5):\n",
    "\n",
    "        num_examples_seen = 0\n",
    "        losses = []\n",
    "        for epoch in range(nepoch):\n",
    "            loss = 0\n",
    "            accuracy = 0\n",
    "\n",
    "            # For each training example...\n",
    "            for i in range(len(Y)):\n",
    "                '''sgd return the loss and accureacy of this sentence/example'''\n",
    "                lss, acc = self.sgd_step(X[i], Y[i], learning_rate)\n",
    "                loss += lss\n",
    "                accuracy += acc\n",
    "                num_examples_seen += 1\n",
    "\n",
    "            if (epoch % evaluate_loss_after == 0):\n",
    "                #1 place\n",
    "                #loss, accuracy = self.calculate_total_loss_and_predict_accuracy(X, Y)\n",
    "                loss /= len(Y)\n",
    "                accuracy /= len(Y)\n",
    "                losses.append(str(float(loss)))\n",
    "                time = datetime.now().strftime('%Y-%m-%d %H:%M:%S')\n",
    "                print(\"%s: Loss after num_examples_seen=%d epoch=%d: %f\" % (time, num_examples_seen, epoch, loss))\n",
    "                f.write(\"%s: Loss after num_examples_seen=%d epoch=%d: %f\\n\" % (time, num_examples_seen, epoch, loss))\n",
    "                #accuracy = self.calculate_total_pred_accuray(X, Y)\n",
    "                print(\"Prediction accuracy=%f\" % (accuracy))\n",
    "                f.write(\"Prediction accuracy=%f\\n\" % (accuracy))\n",
    "                f.flush()\n",
    "                # optional: Adjust the learning rate if loss increases\n",
    "                # if len(losses) > 1 and losses[-1][1] > losses[-2][1]:\n",
    "                #     learning_rate = learning_rate * 0.5\n",
    "                #     print(\"Setting learning rate to %f\" % learning_rate)\n",
    "                sys.stdout.flush()\n",
    "        return losses\n",
    "    \n",
    "    def justForwardPropagation(self, x, y):\n",
    "        x = torch.tensor(x, dtype=torch.int32, device=self.device)\n",
    "        y = torch.tensor(y, dtype=torch.int32, device=self.device)\n",
    "        lss, acc, _, _ = self.forward_propagation(x, y)\n",
    "        return lss, acc\n",
    "\n",
    "    def calculate_total_pred_accuray(self, X, Y):\n",
    "        # For each testing example...\n",
    "        loss = 0\n",
    "        accuracy = 0\n",
    "        for i in range(len(Y)):\n",
    "            lss, acc = self.justForwardPropagation(X[i], Y[i])\n",
    "            loss += lss\n",
    "            accuracy += acc\n",
    "        loss /= len(Y)\n",
    "        accuracy /= len(Y)\n",
    "        print(\"Testing loss value=%f\" % (loss))\n",
    "        f.write(\"Testing loss value=%f\\n\" % (loss))\n",
    "        print(\"Testing prediction accuracy=%f\" % (accuracy))\n",
    "        f.write(\"Testing prediction accuracy=%f\\n\" % (accuracy))\n",
    "        f.flush()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "id": "HVTfFBIdaUps"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "8148\n"
     ]
    }
   ],
   "source": [
    "hidden_dim = 100\n",
    "word_dim = len(vocab)\n",
    "print(word_dim)\n",
    "random_seed = 10 # or any of your favorite number \n",
    "np.random.seed(random_seed)\n",
    "torch.manual_seed(random_seed)\n",
    "torch.cuda.manual_seed(random_seed)\n",
    "torch.backends.cudnn.deterministic = True\n",
    "torch.backends.cudnn.benchmark = False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 355
    },
    "id": "bbyL8jrvabKp",
    "outputId": "5f0d1887-bc17-48f0-b33c-2c6bc340a19a"
   },
   "outputs": [],
   "source": [
    "f = open(\"RNN_orthg_everybp_NLP_log.txt\", \"w\")\n",
    "rnn = Model(f, word_dim, hidden_dim)\n",
    "losses = rnn.train(X_train, Y_train, learning_rate=0.005, nepoch=25, evaluate_loss_after=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "9QkIY3ecaekv"
   },
   "outputs": [],
   "source": [
    "with open(\"RNN_orthg_everybp_NLP_loss_array.txt\", \"w\") as txt_file:\n",
    "  txt_file.write(\", \".join(losses))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "W-ci-7xBag6q",
    "outputId": "8a083245-43cb-4a66-ecbd-badd9644356b"
   },
   "outputs": [],
   "source": [
    "rnn.calculate_total_pred_accuray(X_test, Y_test)\n",
    "f.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "diltDFD5Nwv9"
   },
   "outputs": [],
   "source": [
    "# Convert the PyTorch tensor to a NumPy array\n",
    "numpy_matrix = rnn.U.cpu().numpy()\n",
    "# Save the NumPy array to a CSV file\n",
    "file_path = 'RNN_everybp_NLP_timeSeries_backThrough_U.csv'\n",
    "np.savetxt(file_path, numpy_matrix, delimiter=',')\n",
    "\n",
    "numpy_matrix = rnn.W.cpu().numpy()\n",
    "file_path = 'RNN_everybp_NLP_timeSeries_backThrough_W.csv'\n",
    "np.savetxt(file_path, numpy_matrix, delimiter=',')\n",
    "\n",
    "numpy_matrix = rnn.V.cpu().numpy()\n",
    "file_path = 'RNN_everybp_NLP_timeSeries_backThrough_V.csv'\n",
    "np.savetxt(file_path, numpy_matrix, delimiter=',')\n",
    "\n",
    "numpy_matrix = rnn.b_y.cpu().numpy()\n",
    "file_path = 'RNN_everybp_NLP_timeSeries_backThrough_by.csv'\n",
    "np.savetxt(file_path, numpy_matrix, delimiter=',')\n",
    "\n",
    "numpy_matrix = rnn.b_h.cpu().numpy()\n",
    "file_path = 'RNN_everybp_NLP_timeSeries_backThrough_bh.csv'\n",
    "np.savetxt(file_path, numpy_matrix, delimiter=',')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#continue training\n",
    "learning_rate=0.005\n",
    "nepoch=25\n",
    "f = open(\"RNN_orthg_everybp_NLP_log_part2.txt\", \"w\")\n",
    "X = X_train\n",
    "Y = Y_train\n",
    "\n",
    "num_examples_seen = 200000\n",
    "losses = []\n",
    "for epoch in range(nepoch):\n",
    "    loss = 0\n",
    "    accuracy = 0\n",
    "\n",
    "    # For each training example...\n",
    "    for i in range(len(Y)):\n",
    "        '''sgd return the loss and accureacy of this sentence/example'''\n",
    "        lss, acc = rnn.sgd_step(X[i], Y[i], learning_rate)\n",
    "        loss += lss\n",
    "        accuracy += acc\n",
    "        num_examples_seen += 1\n",
    "        \n",
    "    #1 place\n",
    "    #loss, accuracy = self.calculate_total_loss_and_predict_accuracy(X, Y)\n",
    "    loss /= len(Y)\n",
    "    accuracy /= len(Y)\n",
    "    losses.append(str(float(loss)))\n",
    "    time = datetime.now().strftime('%Y-%m-%d %H:%M:%S')\n",
    "    print(\"%s: Loss after num_examples_seen=%d epoch=%d: %f\" % (time, num_examples_seen, epoch+25, loss))\n",
    "    f.write(\"%s: Loss after num_examples_seen=%d epoch=%d: %f\\n\" % (time, num_examples_seen, epoch+25, loss))\n",
    "    #accuracy = self.calculate_total_pred_accuray(X, Y)\n",
    "    print(\"Prediction accuracy=%f\" % (accuracy))\n",
    "    f.write(\"Prediction accuracy=%f\\n\" % (accuracy))\n",
    "    f.flush()\n",
    "    # optional: Adjust the learning rate if loss increases\n",
    "    # if len(losses) > 1 and losses[-1][1] > losses[-2][1]:\n",
    "    #     learning_rate = learning_rate * 0.5\n",
    "    #     print(\"Setting learning rate to %f\" % learning_rate)\n",
    "    sys.stdout.flush()\n",
    "    \n",
    "#write the loss result into file\n",
    "with open(\"RNN_orthg_everybp_NLP_loss_array_part2.txt\", \"w\") as txt_file:\n",
    "  txt_file.write(\", \".join(losses))\n",
    "    \n",
    "#calculate in test dataset  \n",
    "rnn.calculate_total_pred_accuray(X_test, Y_test)\n",
    "f.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#continue training\n",
    "learning_rate=0.005\n",
    "nepoch=25\n",
    "f = open(\"RNN_orthg_everybp_NLP_log_part3.txt\", \"w\")\n",
    "X = X_train\n",
    "Y = Y_train\n",
    "\n",
    "num_examples_seen = 400000\n",
    "losses = []\n",
    "for epoch in range(nepoch):\n",
    "    loss = 0\n",
    "    accuracy = 0\n",
    "\n",
    "    # For each training example...\n",
    "    for i in range(len(Y)):\n",
    "        '''sgd return the loss and accureacy of this sentence/example'''\n",
    "        lss, acc = rnn.sgd_step(X[i], Y[i], learning_rate)\n",
    "        loss += lss\n",
    "        accuracy += acc\n",
    "        num_examples_seen += 1\n",
    "        \n",
    "    #1 place\n",
    "    #loss, accuracy = self.calculate_total_loss_and_predict_accuracy(X, Y)\n",
    "    loss /= len(Y)\n",
    "    accuracy /= len(Y)\n",
    "    losses.append(str(float(loss)))\n",
    "    time = datetime.now().strftime('%Y-%m-%d %H:%M:%S')\n",
    "    print(\"%s: Loss after num_examples_seen=%d epoch=%d: %f\" % (time, num_examples_seen, epoch+50, loss))\n",
    "    f.write(\"%s: Loss after num_examples_seen=%d epoch=%d: %f\\n\" % (time, num_examples_seen, epoch+50, loss))\n",
    "    #accuracy = self.calculate_total_pred_accuray(X, Y)\n",
    "    print(\"Prediction accuracy=%f\" % (accuracy))\n",
    "    f.write(\"Prediction accuracy=%f\\n\" % (accuracy))\n",
    "    f.flush()\n",
    "    # optional: Adjust the learning rate if loss increases\n",
    "    # if len(losses) > 1 and losses[-1][1] > losses[-2][1]:\n",
    "    #     learning_rate = learning_rate * 0.5\n",
    "    #     print(\"Setting learning rate to %f\" % learning_rate)\n",
    "    sys.stdout.flush()\n",
    "    \n",
    "#write the loss result into file\n",
    "with open(\"RNN_orthg_everybp_NLP_loss_array_part2.txt\", \"w\") as txt_file:\n",
    "  txt_file.write(\", \".join(losses))\n",
    "    \n",
    "#calculate in test dataset  \n",
    "rnn.calculate_total_pred_accuray(X_test, Y_test)\n",
    "f.close()"
   ]
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "gpuType": "T4",
   "provenance": []
  },
  "gpuClass": "standard",
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  },
  "widgets": {
   "application/vnd.jupyter.widget-state+json": {
    "0fd4f0e70dc3492f831b78c12bfc7d11": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "HBoxModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "HBoxModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "HBoxView",
      "box_style": "",
      "children": [
       "IPY_MODEL_6edad33a30784a14b47085d95632422e",
       "IPY_MODEL_72801c57c09644f9b35e6d1010e0ae16",
       "IPY_MODEL_761b8bdf703c4c03abbbd5cc56f5d0b5"
      ],
      "layout": "IPY_MODEL_ed21e4234845474ea15c3d411075cf2a"
     }
    },
    "170d5968dc2042268963f93ecce8b911": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "DescriptionStyleModel",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "DescriptionStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "description_width": ""
     }
    },
    "27689d25627041168afb6dfeb5908b22": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "ProgressStyleModel",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "ProgressStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "bar_color": null,
      "description_width": ""
     }
    },
    "27e38b5fb28e417081c0be0f0698910c": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "59fb6cf5ee994f1ea8aaf5739ee87bb7": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "DescriptionStyleModel",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "DescriptionStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "description_width": ""
     }
    },
    "5be1fc46ac3f45398367cb78794e2446": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "6edad33a30784a14b47085d95632422e": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "HTMLModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "HTMLModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "HTMLView",
      "description": "",
      "description_tooltip": null,
      "layout": "IPY_MODEL_5be1fc46ac3f45398367cb78794e2446",
      "placeholder": "​",
      "style": "IPY_MODEL_170d5968dc2042268963f93ecce8b911",
      "value": "100%"
     }
    },
    "72801c57c09644f9b35e6d1010e0ae16": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "FloatProgressModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "FloatProgressModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "ProgressView",
      "bar_style": "success",
      "description": "",
      "description_tooltip": null,
      "layout": "IPY_MODEL_f1afce97b62a487a9f7b47f49a47a9cc",
      "max": 3,
      "min": 0,
      "orientation": "horizontal",
      "style": "IPY_MODEL_27689d25627041168afb6dfeb5908b22",
      "value": 3
     }
    },
    "761b8bdf703c4c03abbbd5cc56f5d0b5": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "HTMLModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "HTMLModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "HTMLView",
      "description": "",
      "description_tooltip": null,
      "layout": "IPY_MODEL_27e38b5fb28e417081c0be0f0698910c",
      "placeholder": "​",
      "style": "IPY_MODEL_59fb6cf5ee994f1ea8aaf5739ee87bb7",
      "value": " 3/3 [00:00&lt;00:00, 132.65it/s]"
     }
    },
    "ed21e4234845474ea15c3d411075cf2a": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "f1afce97b62a487a9f7b47f49a47a9cc": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    }
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
